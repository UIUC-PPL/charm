# Jacobi3D Performance Benchmarking
# Block size 1024 x 1024 x 512
# Iteration 1
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 24576 -Y 8192 -Z 4096 -x 1024 -y 1024 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.005 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 1024 x 1024 x 512, Chares: 24 x 8 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.893 s
Total time: 5.218 s
Average iteration time: 52177.600 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 24576 -Y 8192 -Z 4096 -x 1024 -y 1024 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 1024 x 1024 x 512, Chares: 24 x 8 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.910 s
Total time: 5.236 s
Average iteration time: 52363.331 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 24576 -Y 8192 -Z 4096 -x 1024 -y 1024 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 1024 x 1024 x 512, Chares: 24 x 8 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.792 s
Total time: 5.220 s
Average iteration time: 52199.324 us
[Partition 0][Node 0] End of program
# Block size 1024 x 512 x 512
# Iteration 1
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 24576 -Y 8192 -Z 4096 -x 1024 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 1024 x 512 x 512, Chares: 24 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.714 s
Total time: 5.014 s
Average iteration time: 50142.360 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 24576 -Y 8192 -Z 4096 -x 1024 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 1024 x 512 x 512, Chares: 24 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.832 s
Total time: 5.085 s
Average iteration time: 50853.966 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 24576 -Y 8192 -Z 4096 -x 1024 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 1024 x 512 x 512, Chares: 24 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.963 s
Total time: 5.082 s
Average iteration time: 50824.949 us
[Partition 0][Node 0] End of program
# Block size 512 x 512 x 512
# Iteration 1
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 24576 -Y 8192 -Z 4096 -x 512 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 512 x 512, Chares: 48 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.809 s
Total time: 4.059 s
Average iteration time: 40591.370 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 24576 -Y 8192 -Z 4096 -x 512 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 512 x 512, Chares: 48 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.825 s
Total time: 4.062 s
Average iteration time: 40616.802 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 24576 -Y 8192 -Z 4096 -x 512 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 512 x 512, Chares: 48 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.832 s
Total time: 4.056 s
Average iteration time: 40561.314 us
[Partition 0][Node 0] End of program
# Block size 512 x 512 x 256
# Iteration 1
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 24576 -Y 8192 -Z 4096 -x 512 -y 512 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 512 x 256, Chares: 48 x 16 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 3.071 s
Total time: 4.828 s
Average iteration time: 48283.103 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 24576 -Y 8192 -Z 4096 -x 512 -y 512 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 512 x 256, Chares: 48 x 16 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.922 s
Total time: 4.750 s
Average iteration time: 47498.822 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 24576 -Y 8192 -Z 4096 -x 512 -y 512 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 512 x 256, Chares: 48 x 16 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.876 s
Total time: 4.702 s
Average iteration time: 47015.120 us
[Partition 0][Node 0] End of program
# Block size 512 x 256 x 256
# Iteration 1
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 24576 -Y 8192 -Z 4096 -x 512 -y 256 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 256 x 256, Chares: 48 x 32 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.935 s
Total time: 5.937 s
Average iteration time: 59373.174 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 24576 -Y 8192 -Z 4096 -x 512 -y 256 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 256 x 256, Chares: 48 x 32 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 3.070 s
Total time: 5.911 s
Average iteration time: 59106.319 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 24576 -Y 8192 -Z 4096 -x 512 -y 256 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 256 x 256, Chares: 48 x 32 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 3.067 s
Total time: 5.921 s
Average iteration time: 59209.055 us
[Partition 0][Node 0] End of program

------------------------------------------------------------
Sender: LSF System <lsfadmin@batch1>
Subject: Job 277405: <jacobi3d-s-n256> in cluster <summit> Done

Job <jacobi3d-s-n256> was submitted from host <login4> by user <jchoi> in cluster <summit> at Wed Aug 12 23:41:08 2020
Job was executed on host(s) <1*batch1>, in queue <batch>, as user <jchoi> in cluster <summit> at Thu Aug 13 00:23:31 2020
                            <42*b32n06>
                            <42*b32n07>
                            <42*b32n08>
                            <42*b32n09>
                            <42*b32n10>
                            <42*b32n11>
                            <42*b32n12>
                            <42*b32n13>
                            <42*b32n14>
                            <42*b32n15>
                            <42*b32n16>
                            <42*b32n17>
                            <42*b32n18>
                            <42*b33n01>
                            <42*b33n02>
                            <42*b33n03>
                            <42*b33n04>
                            <42*b33n05>
                            <42*b33n06>
                            <42*b33n07>
                            <42*b33n08>
                            <42*b33n09>
                            <42*b33n10>
                            <42*b33n11>
                            <42*b33n12>
                            <42*b33n13>
                            <42*b33n14>
                            <42*b33n15>
                            <42*b33n16>
                            <42*b33n17>
                            <42*b33n18>
                            <42*b34n01>
                            <42*b34n02>
                            <42*b34n03>
                            <42*b34n04>
                            <42*b34n05>
                            <42*b34n06>
                            <42*b34n07>
                            <42*b34n08>
                            <42*b34n09>
                            <42*b34n10>
                            <42*b34n11>
                            <42*b34n12>
                            <42*b34n13>
                            <42*b34n14>
                            <42*b34n15>
                            <42*b34n16>
                            <42*b34n17>
                            <42*b34n18>
                            <42*b35n01>
                            <42*b35n02>
                            <42*b35n03>
                            <42*b35n04>
                            <42*b35n05>
                            <42*b35n06>
                            <42*b35n07>
                            <42*b35n08>
                            <42*b35n09>
                            <42*b35n10>
                            <42*b35n11>
                            <42*b35n12>
                            <42*b35n13>
                            <42*b35n14>
                            <42*b35n15>
                            <42*b35n16>
                            <42*b35n17>
                            <42*b35n18>
                            <42*b36n01>
                            <42*b36n02>
                            <42*b36n03>
                            <42*b36n04>
                            <42*b36n05>
                            <42*b36n06>
                            <42*b36n07>
                            <42*b36n08>
                            <42*b36n09>
                            <42*b36n10>
                            <42*b36n11>
                            <42*b36n12>
                            <42*b36n13>
                            <42*b36n14>
                            <42*b36n15>
                            <42*b36n16>
                            <42*b36n17>
                            <42*b36n18>
                            <42*c01n01>
                            <42*c01n02>
                            <42*c01n03>
                            <42*c01n04>
                            <42*c01n05>
                            <42*c01n06>
                            <42*c01n07>
                            <42*c01n08>
                            <42*c01n09>
                            <42*c01n10>
                            <42*c01n11>
                            <42*c01n12>
                            <42*c01n13>
                            <42*c01n14>
                            <42*c01n15>
                            <42*c01n16>
                            <42*c01n17>
                            <42*c01n18>
                            <42*c02n01>
                            <42*c02n02>
                            <42*c02n03>
                            <42*c02n04>
                            <42*c02n05>
                            <42*c02n06>
                            <42*c02n07>
                            <42*c02n08>
                            <42*c02n09>
                            <42*c02n10>
                            <42*c02n11>
                            <42*c02n12>
                            <42*c02n13>
                            <42*c02n14>
                            <42*c02n15>
                            <42*c02n16>
                            <42*c02n17>
                            <42*c02n18>
                            <42*c03n01>
                            <42*c03n02>
                            <42*c03n03>
                            <42*c03n04>
                            <42*c03n05>
                            <42*c03n06>
                            <42*c03n07>
                            <42*c03n08>
                            <42*c03n09>
                            <42*c03n10>
                            <42*c03n11>
                            <42*c03n12>
                            <42*c03n13>
                            <42*c03n14>
                            <42*c03n15>
                            <42*c09n03>
                            <42*c09n04>
                            <42*c09n05>
                            <42*c09n06>
                            <42*c09n07>
                            <42*c09n08>
                            <42*c09n09>
                            <42*c09n10>
                            <42*c09n11>
                            <42*c09n12>
                            <42*c09n13>
                            <42*c09n14>
                            <42*c09n15>
                            <42*c09n16>
                            <42*c09n17>
                            <42*c09n18>
                            <42*c10n01>
                            <42*c10n02>
                            <42*c10n03>
                            <42*c10n04>
                            <42*c10n05>
                            <42*c10n06>
                            <42*c10n07>
                            <42*c10n08>
                            <42*c10n09>
                            <42*c10n10>
                            <42*c10n11>
                            <42*c10n12>
                            <42*c10n13>
                            <42*c10n14>
                            <42*c10n15>
                            <42*c10n16>
                            <42*c10n17>
                            <42*c10n18>
                            <42*c11n01>
                            <42*c11n02>
                            <42*c11n03>
                            <42*c11n04>
                            <42*c11n05>
                            <42*c11n06>
                            <42*c11n07>
                            <42*c11n08>
                            <42*c11n09>
                            <42*c11n10>
                            <42*c11n11>
                            <42*c11n12>
                            <42*c11n13>
                            <42*c11n14>
                            <42*c11n15>
                            <42*c11n16>
                            <42*c11n17>
                            <42*c11n18>
                            <42*c12n01>
                            <42*c12n02>
                            <42*c12n03>
                            <42*c12n04>
                            <42*c12n05>
                            <42*c12n06>
                            <42*c12n07>
                            <42*c12n08>
                            <42*c12n09>
                            <42*c13n01>
                            <42*c13n02>
                            <42*c13n03>
                            <42*c13n04>
                            <42*c13n05>
                            <42*c13n06>
                            <42*c13n07>
                            <42*c13n08>
                            <42*c13n09>
                            <42*c13n10>
                            <42*c13n11>
                            <42*c13n12>
                            <42*c13n13>
                            <42*c13n14>
                            <42*c13n15>
                            <42*c13n16>
                            <42*c13n17>
                            <42*c13n18>
                            <42*c25n01>
                            <42*c25n02>
                            <42*c25n03>
                            <42*c25n04>
                            <42*c25n05>
                            <42*c25n06>
                            <42*c25n07>
                            <42*c25n08>
                            <42*c25n09>
                            <42*c25n10>
                            <42*c25n11>
                            <42*c25n12>
                            <42*c25n13>
                            <42*c25n14>
                            <42*c25n15>
                            <42*c25n16>
                            <42*c25n17>
                            <42*c25n18>
                            <42*c26n03>
                            <42*c26n05>
                            <42*c26n06>
                            <42*c26n07>
                            <42*c26n08>
                            <42*c26n09>
                            <42*c26n10>
                            <42*c26n11>
                            <42*c26n12>
                            <42*c26n13>
                            <42*c26n14>
                            <42*c28n11>
                            <42*e29n05>
                            <42*g21n16>
                            <42*g22n18>
                            <42*g23n01>
                            <42*g23n02>
                            <42*g23n03>
                            <42*g23n04>
                            <42*g23n05>
                            <42*g23n06>
                            <42*g23n07>
                            <42*g23n08>
</ccs/home/jchoi> was used as the home directory.
</ccs/home/jchoi/work/charm/examples/charm++/cuda/gpudirect/jacobi3d/scripts/summit> was used as the working directory.
Started at Thu Aug 13 00:23:31 2020
Terminated at Thu Aug 13 00:27:06 2020
Results reported at Thu Aug 13 00:27:06 2020

The output (if any) is above this job summary.

