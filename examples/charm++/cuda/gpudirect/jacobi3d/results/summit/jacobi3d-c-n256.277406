# Jacobi3D Performance Benchmarking
# Block size 1024 x 1024 x 512
# Iteration 1
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-c -X 24576 -Y 8192 -Z 4096 -x 1024 -y 1024 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-407-g331b884
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 1024 x 1024 x 512, Chares: 24 x 8 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.831 s
Total time: 3.794 s
Average iteration time: 37942.697 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-c -X 24576 -Y 8192 -Z 4096 -x 1024 -y 1024 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-407-g331b884
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 1024 x 1024 x 512, Chares: 24 x 8 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.854 s
Total time: 3.795 s
Average iteration time: 37946.759 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-c -X 24576 -Y 8192 -Z 4096 -x 1024 -y 1024 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-407-g331b884
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 1024 x 1024 x 512, Chares: 24 x 8 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.719 s
Total time: 3.763 s
Average iteration time: 37633.966 us
[Partition 0][Node 0] End of program
# Block size 1024 x 512 x 512
# Iteration 1
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-c -X 24576 -Y 8192 -Z 4096 -x 1024 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-407-g331b884
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 1024 x 512 x 512, Chares: 24 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.771 s
Total time: 2.817 s
Average iteration time: 28170.061 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-c -X 24576 -Y 8192 -Z 4096 -x 1024 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-407-g331b884
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 1024 x 512 x 512, Chares: 24 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.787 s
Total time: 2.759 s
Average iteration time: 27587.357 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-c -X 24576 -Y 8192 -Z 4096 -x 1024 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-407-g331b884
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 1024 x 512 x 512, Chares: 24 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.855 s
Total time: 2.794 s
Average iteration time: 27938.747 us
[Partition 0][Node 0] End of program
# Block size 512 x 512 x 512
# Iteration 1
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-c -X 24576 -Y 8192 -Z 4096 -x 512 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-407-g331b884
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 512 x 512, Chares: 48 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.820 s
Total time: 2.982 s
Average iteration time: 29819.004 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-c -X 24576 -Y 8192 -Z 4096 -x 512 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-407-g331b884
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 512 x 512, Chares: 48 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.888 s
Total time: 2.973 s
Average iteration time: 29733.966 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-c -X 24576 -Y 8192 -Z 4096 -x 512 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-407-g331b884
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 512 x 512, Chares: 48 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.697 s
Total time: 2.944 s
Average iteration time: 29435.849 us
[Partition 0][Node 0] End of program
# Block size 512 x 512 x 256
# Iteration 1
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-c -X 24576 -Y 8192 -Z 4096 -x 512 -y 512 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-407-g331b884
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.005 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 512 x 256, Chares: 48 x 16 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.827 s
Total time: 3.578 s
Average iteration time: 35781.923 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-c -X 24576 -Y 8192 -Z 4096 -x 512 -y 512 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-407-g331b884
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 512 x 256, Chares: 48 x 16 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.931 s
Total time: 3.576 s
Average iteration time: 35758.423 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-c -X 24576 -Y 8192 -Z 4096 -x 512 -y 512 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-407-g331b884
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 512 x 256, Chares: 48 x 16 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.868 s
Total time: 3.574 s
Average iteration time: 35740.947 us
[Partition 0][Node 0] End of program
# Block size 512 x 256 x 256
# Iteration 1
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-c -X 24576 -Y 8192 -Z 4096 -x 512 -y 256 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-407-g331b884
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 256 x 256, Chares: 48 x 32 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.978 s
Total time: 4.805 s
Average iteration time: 48054.763 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-c -X 24576 -Y 8192 -Z 4096 -x 512 -y 256 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-407-g331b884
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 256 x 256, Chares: 48 x 32 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.914 s
Total time: 4.886 s
Average iteration time: 48856.525 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-c -X 24576 -Y 8192 -Z 4096 -x 512 -y 256 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-407-g331b884
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 256 x 256, Chares: 48 x 32 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 4.840 s
Total time: 4.840 s
Average iteration time: 48395.961 us
[Partition 0][Node 0] End of program

------------------------------------------------------------
Sender: LSF System <lsfadmin@batch5>
Subject: Job 277406: <jacobi3d-c-n256> in cluster <summit> Done

Job <jacobi3d-c-n256> was submitted from host <login4> by user <jchoi> in cluster <summit> at Wed Aug 12 23:41:21 2020
Job was executed on host(s) <1*batch5>, in queue <batch>, as user <jchoi> in cluster <summit> at Thu Aug 13 00:19:46 2020
                            <42*a01n01>
                            <42*a01n02>
                            <42*a01n03>
                            <42*a01n04>
                            <42*a01n05>
                            <42*a01n06>
                            <42*a01n07>
                            <42*a01n08>
                            <42*a01n09>
                            <42*a01n10>
                            <42*a01n11>
                            <42*a01n12>
                            <42*a01n13>
                            <42*a01n14>
                            <42*a01n15>
                            <42*a01n16>
                            <42*a01n17>
                            <42*a01n18>
                            <42*a02n01>
                            <42*a02n02>
                            <42*a02n03>
                            <42*a02n04>
                            <42*a02n05>
                            <42*a02n06>
                            <42*a02n07>
                            <42*a02n08>
                            <42*a02n09>
                            <42*a02n10>
                            <42*a02n11>
                            <42*a02n12>
                            <42*a02n13>
                            <42*a02n14>
                            <42*a02n15>
                            <42*a02n16>
                            <42*a02n17>
                            <42*a02n18>
                            <42*a03n01>
                            <42*a03n02>
                            <42*a03n03>
                            <42*a03n04>
                            <42*a03n05>
                            <42*a03n06>
                            <42*a03n07>
                            <42*a03n08>
                            <42*a03n09>
                            <42*a03n10>
                            <42*a03n11>
                            <42*a03n12>
                            <42*a03n13>
                            <42*a04n11>
                            <42*a04n12>
                            <42*a05n06>
                            <42*a05n07>
                            <42*a05n08>
                            <42*a05n09>
                            <42*a05n10>
                            <42*a05n11>
                            <42*a05n12>
                            <42*a05n13>
                            <42*a06n17>
                            <42*a06n18>
                            <42*a07n03>
                            <42*a07n04>
                            <42*a10n04>
                            <42*a11n07>
                            <42*a12n02>
                            <42*a12n03>
                            <42*a12n04>
                            <42*a12n05>
                            <42*a12n06>
                            <42*a12n07>
                            <42*a12n08>
                            <42*a12n09>
                            <42*a12n10>
                            <42*a12n11>
                            <42*a12n12>
                            <42*a12n13>
                            <42*a12n14>
                            <42*a12n15>
                            <42*a12n16>
                            <42*a12n17>
                            <42*a12n18>
                            <42*a13n01>
                            <42*a13n02>
                            <42*a13n03>
                            <42*a13n04>
                            <42*a13n05>
                            <42*a13n06>
                            <42*a13n07>
                            <42*a13n08>
                            <42*a13n09>
                            <42*a13n10>
                            <42*a13n11>
                            <42*a13n12>
                            <42*a13n13>
                            <42*a13n14>
                            <42*a13n15>
                            <42*a13n16>
                            <42*a13n17>
                            <42*a13n18>
                            <42*a14n01>
                            <42*a14n02>
                            <42*a14n03>
                            <42*a14n04>
                            <42*a14n05>
                            <42*a14n06>
                            <42*a14n07>
                            <42*a14n08>
                            <42*a14n09>
                            <42*a14n10>
                            <42*a14n11>
                            <42*a14n12>
                            <42*a14n13>
                            <42*a14n14>
                            <42*a14n15>
                            <42*a14n16>
                            <42*a14n17>
                            <42*a14n18>
                            <42*a15n02>
                            <42*a15n03>
                            <42*a15n04>
                            <42*a15n05>
                            <42*a15n06>
                            <42*a15n07>
                            <42*a15n08>
                            <42*a15n09>
                            <42*a15n10>
                            <42*a15n11>
                            <42*a15n12>
                            <42*a15n13>
                            <42*a15n14>
                            <42*a15n15>
                            <42*a15n16>
                            <42*a15n17>
                            <42*a15n18>
                            <42*a16n01>
                            <42*a16n02>
                            <42*a16n03>
                            <42*a16n04>
                            <42*a16n05>
                            <42*a16n06>
                            <42*a16n07>
                            <42*a16n08>
                            <42*a16n09>
                            <42*a16n10>
                            <42*a16n11>
                            <42*a16n12>
                            <42*a16n13>
                            <42*a16n14>
                            <42*a16n15>
                            <42*a16n16>
                            <42*a16n17>
                            <42*a16n18>
                            <42*a17n01>
                            <42*a17n02>
                            <42*a17n03>
                            <42*a17n04>
                            <42*a17n05>
                            <42*a17n06>
                            <42*a17n07>
                            <42*a17n08>
                            <42*a17n09>
                            <42*a17n10>
                            <42*a17n11>
                            <42*a17n12>
                            <42*a17n13>
                            <42*a17n14>
                            <42*a17n15>
                            <42*a17n16>
                            <42*a17n17>
                            <42*a17n18>
                            <42*a18n01>
                            <42*a18n02>
                            <42*a18n03>
                            <42*a18n04>
                            <42*a18n05>
                            <42*a18n06>
                            <42*a18n07>
                            <42*a18n08>
                            <42*a18n09>
                            <42*a18n10>
                            <42*a18n11>
                            <42*a18n12>
                            <42*a18n13>
                            <42*a18n14>
                            <42*a18n15>
                            <42*a18n16>
                            <42*a18n17>
                            <42*a18n18>
                            <42*a19n01>
                            <42*a19n02>
                            <42*a19n03>
                            <42*a19n04>
                            <42*a19n05>
                            <42*a19n06>
                            <42*a19n07>
                            <42*a19n08>
                            <42*a19n09>
                            <42*a19n10>
                            <42*a19n11>
                            <42*a19n12>
                            <42*a19n13>
                            <42*a19n14>
                            <42*a19n15>
                            <42*a19n16>
                            <42*a19n17>
                            <42*a19n18>
                            <42*a20n01>
                            <42*a20n02>
                            <42*a20n03>
                            <42*a20n04>
                            <42*a20n05>
                            <42*a20n06>
                            <42*a20n07>
                            <42*a20n08>
                            <42*a20n09>
                            <42*a20n10>
                            <42*a20n11>
                            <42*a20n12>
                            <42*a20n13>
                            <42*a20n14>
                            <42*a20n15>
                            <42*a20n16>
                            <42*a20n17>
                            <42*a20n18>
                            <42*a21n01>
                            <42*a21n02>
                            <42*a21n03>
                            <42*a21n04>
                            <42*a21n05>
                            <42*a21n06>
                            <42*a21n07>
                            <42*a21n08>
                            <42*a21n09>
                            <42*a21n10>
                            <42*a21n11>
                            <42*a21n12>
                            <42*a21n13>
                            <42*a21n14>
                            <42*a21n15>
                            <42*a21n16>
                            <42*a21n17>
                            <42*a21n18>
                            <42*a22n01>
                            <42*a22n02>
                            <42*a22n03>
                            <42*a22n04>
                            <42*a22n05>
                            <42*a22n06>
                            <42*a22n07>
                            <42*a22n08>
                            <42*a22n09>
                            <42*a22n10>
                            <42*a22n11>
                            <42*a22n12>
                            <42*a22n13>
</ccs/home/jchoi> was used as the home directory.
</ccs/home/jchoi/work/charm/examples/charm++/cuda/gpudirect/jacobi3d/scripts/summit> was used as the working directory.
Started at Thu Aug 13 00:19:46 2020
Terminated at Thu Aug 13 00:22:54 2020
Results reported at Thu Aug 13 00:22:54 2020

The output (if any) is above this job summary.

