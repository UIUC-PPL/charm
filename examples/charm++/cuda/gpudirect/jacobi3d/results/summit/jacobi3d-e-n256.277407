# Jacobi3D Performance Benchmarking
# Block size 1024 x 1024 x 512
# Iteration 1
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 24576 -Y 8192 -Z 4096 -x 1024 -y 1024 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 1024 x 1024 x 512, Chares: 24 x 8 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.941 s
Total time: 3.692 s
Average iteration time: 36917.690 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 24576 -Y 8192 -Z 4096 -x 1024 -y 1024 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 1024 x 1024 x 512, Chares: 24 x 8 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.969 s
Total time: 3.706 s
Average iteration time: 37062.820 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 24576 -Y 8192 -Z 4096 -x 1024 -y 1024 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 1024 x 1024 x 512, Chares: 24 x 8 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.711 s
Total time: 3.702 s
Average iteration time: 37024.904 us
[Partition 0][Node 0] End of program
# Block size 1024 x 512 x 512
# Iteration 1
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 24576 -Y 8192 -Z 4096 -x 1024 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 1024 x 512 x 512, Chares: 24 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.833 s
Total time: 2.639 s
Average iteration time: 26386.722 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 24576 -Y 8192 -Z 4096 -x 1024 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 1024 x 512 x 512, Chares: 24 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.657 s
Total time: 2.631 s
Average iteration time: 26313.509 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 24576 -Y 8192 -Z 4096 -x 1024 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 1024 x 512 x 512, Chares: 24 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.707 s
Total time: 2.643 s
Average iteration time: 26425.099 us
[Partition 0][Node 0] End of program
# Block size 512 x 512 x 512
# Iteration 1
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 24576 -Y 8192 -Z 4096 -x 512 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 512 x 512, Chares: 48 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.922 s
Total time: 2.922 s
Average iteration time: 29216.350 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 24576 -Y 8192 -Z 4096 -x 512 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 512 x 512, Chares: 48 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.727 s
Total time: 2.920 s
Average iteration time: 29202.431 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 24576 -Y 8192 -Z 4096 -x 512 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 512 x 512, Chares: 48 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.743 s
Total time: 2.938 s
Average iteration time: 29378.770 us
[Partition 0][Node 0] End of program
# Block size 512 x 512 x 256
# Iteration 1
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 24576 -Y 8192 -Z 4096 -x 512 -y 512 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 512 x 256, Chares: 48 x 16 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.876 s
Total time: 3.583 s
Average iteration time: 35829.428 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 24576 -Y 8192 -Z 4096 -x 512 -y 512 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 512 x 256, Chares: 48 x 16 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.895 s
Total time: 3.561 s
Average iteration time: 35609.451 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 24576 -Y 8192 -Z 4096 -x 512 -y 512 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 512 x 256, Chares: 48 x 16 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.965 s
Total time: 3.551 s
Average iteration time: 35509.914 us
[Partition 0][Node 0] End of program
# Block size 512 x 256 x 256
# Iteration 1
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 24576 -Y 8192 -Z 4096 -x 512 -y 256 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 256 x 256, Chares: 48 x 32 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.932 s
Total time: 4.753 s
Average iteration time: 47527.201 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 24576 -Y 8192 -Z 4096 -x 512 -y 256 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 256 x 256, Chares: 48 x 32 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.956 s
Total time: 4.757 s
Average iteration time: 47567.353 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n1536 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 24576 -Y 8192 -Z 4096 -x 512 -y 256 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 1536 processes, 1 worker threads (PEs) + 0 comm threads per process, 1536 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 256 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 24576 x 8192 x 4096, Block: 512 x 256 x 256, Chares: 48 x 32 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 3.007 s
Total time: 4.751 s
Average iteration time: 47514.703 us
[Partition 0][Node 0] End of program

------------------------------------------------------------
Sender: LSF System <lsfadmin@batch5>
Subject: Job 277407: <jacobi3d-e-n256> in cluster <summit> Done

Job <jacobi3d-e-n256> was submitted from host <login4> by user <jchoi> in cluster <summit> at Wed Aug 12 23:41:28 2020
Job was executed on host(s) <1*batch5>, in queue <batch>, as user <jchoi> in cluster <summit> at Thu Aug 13 00:19:58 2020
                            <42*a22n14>
                            <42*a22n15>
                            <42*a22n16>
                            <42*a22n17>
                            <42*a22n18>
                            <42*a23n01>
                            <42*a23n02>
                            <42*a23n03>
                            <42*a23n04>
                            <42*a23n05>
                            <42*a23n06>
                            <42*a23n07>
                            <42*a23n08>
                            <42*a23n09>
                            <42*a23n10>
                            <42*a23n11>
                            <42*a23n12>
                            <42*a23n13>
                            <42*a23n14>
                            <42*a23n15>
                            <42*a23n16>
                            <42*a23n17>
                            <42*a23n18>
                            <42*a24n01>
                            <42*a24n02>
                            <42*a24n03>
                            <42*a24n04>
                            <42*a24n05>
                            <42*a24n06>
                            <42*a24n07>
                            <42*a24n08>
                            <42*a24n09>
                            <42*a24n10>
                            <42*a24n11>
                            <42*a24n12>
                            <42*a24n13>
                            <42*a24n14>
                            <42*a24n15>
                            <42*a24n16>
                            <42*a24n17>
                            <42*a24n18>
                            <42*a25n01>
                            <42*a25n02>
                            <42*a25n03>
                            <42*a25n04>
                            <42*a25n05>
                            <42*a25n06>
                            <42*a25n07>
                            <42*a25n08>
                            <42*a25n09>
                            <42*a25n10>
                            <42*a25n11>
                            <42*a25n12>
                            <42*a25n13>
                            <42*a25n14>
                            <42*a25n15>
                            <42*a25n16>
                            <42*a25n17>
                            <42*a25n18>
                            <42*a26n01>
                            <42*a26n02>
                            <42*a26n03>
                            <42*a26n04>
                            <42*a26n05>
                            <42*a26n06>
                            <42*a26n07>
                            <42*a26n08>
                            <42*a26n09>
                            <42*a26n10>
                            <42*a26n11>
                            <42*a26n12>
                            <42*a26n13>
                            <42*a26n14>
                            <42*a26n15>
                            <42*a26n16>
                            <42*a26n17>
                            <42*a26n18>
                            <42*a27n01>
                            <42*a27n02>
                            <42*a27n03>
                            <42*a27n04>
                            <42*a27n05>
                            <42*a27n06>
                            <42*a27n07>
                            <42*a27n08>
                            <42*a27n09>
                            <42*a27n10>
                            <42*a27n11>
                            <42*a27n12>
                            <42*a27n13>
                            <42*a27n14>
                            <42*a27n15>
                            <42*a27n16>
                            <42*a27n17>
                            <42*a27n18>
                            <42*a28n01>
                            <42*a28n02>
                            <42*a28n03>
                            <42*a28n04>
                            <42*a28n05>
                            <42*a28n06>
                            <42*a28n07>
                            <42*a28n08>
                            <42*a28n09>
                            <42*a28n10>
                            <42*a28n11>
                            <42*a28n12>
                            <42*a28n13>
                            <42*a29n11>
                            <42*a29n12>
                            <42*a29n13>
                            <42*a29n14>
                            <42*a29n15>
                            <42*a29n16>
                            <42*a29n17>
                            <42*a29n18>
                            <42*a30n01>
                            <42*a30n02>
                            <42*a30n03>
                            <42*a30n04>
                            <42*a30n05>
                            <42*a30n06>
                            <42*a30n07>
                            <42*a30n08>
                            <42*a30n09>
                            <42*a30n10>
                            <42*a30n11>
                            <42*a30n12>
                            <42*a30n13>
                            <42*a30n14>
                            <42*a30n15>
                            <42*a30n16>
                            <42*a30n17>
                            <42*a30n18>
                            <42*a31n01>
                            <42*a31n02>
                            <42*a31n03>
                            <42*a31n04>
                            <42*a31n05>
                            <42*a31n06>
                            <42*a31n07>
                            <42*a31n08>
                            <42*a31n09>
                            <42*a31n10>
                            <42*a31n11>
                            <42*a31n12>
                            <42*a31n13>
                            <42*a31n14>
                            <42*a31n15>
                            <42*a31n16>
                            <42*a31n17>
                            <42*a31n18>
                            <42*a32n01>
                            <42*a32n02>
                            <42*a32n03>
                            <42*a32n04>
                            <42*a32n05>
                            <42*a32n06>
                            <42*a32n07>
                            <42*a32n08>
                            <42*a32n09>
                            <42*a32n10>
                            <42*a32n11>
                            <42*a32n12>
                            <42*a32n13>
                            <42*a32n14>
                            <42*a32n15>
                            <42*a32n16>
                            <42*a32n17>
                            <42*a32n18>
                            <42*a33n01>
                            <42*a33n02>
                            <42*a33n03>
                            <42*a33n04>
                            <42*a33n05>
                            <42*a33n06>
                            <42*a33n07>
                            <42*a33n08>
                            <42*a33n09>
                            <42*a33n11>
                            <42*a33n12>
                            <42*a33n13>
                            <42*a33n14>
                            <42*a33n15>
                            <42*a33n16>
                            <42*a33n17>
                            <42*a33n18>
                            <42*a34n01>
                            <42*a34n03>
                            <42*a34n04>
                            <42*a34n05>
                            <42*a34n06>
                            <42*a34n07>
                            <42*a34n08>
                            <42*a34n09>
                            <42*a34n10>
                            <42*a34n11>
                            <42*a34n12>
                            <42*a34n13>
                            <42*a34n14>
                            <42*a34n15>
                            <42*a34n16>
                            <42*a34n17>
                            <42*a34n18>
                            <42*a35n01>
                            <42*a35n02>
                            <42*a35n03>
                            <42*a35n04>
                            <42*a35n05>
                            <42*a35n06>
                            <42*a35n07>
                            <42*a35n08>
                            <42*a35n09>
                            <42*a35n10>
                            <42*a35n11>
                            <42*a35n12>
                            <42*a35n13>
                            <42*a35n14>
                            <42*a35n15>
                            <42*a35n16>
                            <42*a35n17>
                            <42*a35n18>
                            <42*a36n01>
                            <42*a36n02>
                            <42*a36n03>
                            <42*a36n04>
                            <42*a36n05>
                            <42*a36n06>
                            <42*a36n07>
                            <42*a36n08>
                            <42*a36n09>
                            <42*a36n10>
                            <42*a36n11>
                            <42*a36n12>
                            <42*a36n13>
                            <42*a36n14>
                            <42*a36n15>
                            <42*a36n16>
                            <42*a36n17>
                            <42*a36n18>
                            <42*b01n01>
                            <42*b01n02>
                            <42*b01n03>
                            <42*b01n04>
                            <42*b01n05>
                            <42*b01n07>
                            <42*b01n08>
                            <42*b01n09>
                            <42*b01n10>
                            <42*b01n11>
                            <42*b01n12>
                            <42*b01n13>
                            <42*b01n14>
                            <42*b01n15>
                            <42*b01n16>
                            <42*b01n17>
</ccs/home/jchoi> was used as the home directory.
</ccs/home/jchoi/work/charm/examples/charm++/cuda/gpudirect/jacobi3d/scripts/summit> was used as the working directory.
Started at Thu Aug 13 00:19:58 2020
Terminated at Thu Aug 13 00:23:03 2020
Results reported at Thu Aug 13 00:23:03 2020

The output (if any) is above this job summary.

