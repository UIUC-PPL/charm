# Jacobi3D Performance Benchmarking
# Block size 1024 x 1024 x 512
# Iteration 1
$ jsrun -n192 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 4096 -Z 2048 -x 1024 -y 1024 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 192 processes, 1 worker threads (PEs) + 0 comm threads per process, 192 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 32 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 4096 x 2048, Block: 1024 x 1024 x 512, Chares: 12 x 4 x 4, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.762 s
Total time: 3.433 s
Average iteration time: 34333.015 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n192 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 4096 -Z 2048 -x 1024 -y 1024 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 192 processes, 1 worker threads (PEs) + 0 comm threads per process, 192 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 32 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.003 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 4096 x 2048, Block: 1024 x 1024 x 512, Chares: 12 x 4 x 4, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.949 s
Total time: 3.493 s
Average iteration time: 34929.908 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n192 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 4096 -Z 2048 -x 1024 -y 1024 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 192 processes, 1 worker threads (PEs) + 0 comm threads per process, 192 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 32 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 4096 x 2048, Block: 1024 x 1024 x 512, Chares: 12 x 4 x 4, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.692 s
Total time: 3.493 s
Average iteration time: 34925.796 us
[Partition 0][Node 0] End of program
# Block size 1024 x 512 x 512
# Iteration 1
$ jsrun -n192 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 4096 -Z 2048 -x 1024 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 192 processes, 1 worker threads (PEs) + 0 comm threads per process, 192 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 32 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 4096 x 2048, Block: 1024 x 512 x 512, Chares: 12 x 8 x 4, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.624 s
Total time: 2.380 s
Average iteration time: 23804.029 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n192 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 4096 -Z 2048 -x 1024 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 192 processes, 1 worker threads (PEs) + 0 comm threads per process, 192 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 32 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 4096 x 2048, Block: 1024 x 512 x 512, Chares: 12 x 8 x 4, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.712 s
Total time: 2.348 s
Average iteration time: 23480.885 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n192 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 4096 -Z 2048 -x 1024 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 192 processes, 1 worker threads (PEs) + 0 comm threads per process, 192 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 32 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 4096 x 2048, Block: 1024 x 512 x 512, Chares: 12 x 8 x 4, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.676 s
Total time: 2.372 s
Average iteration time: 23724.051 us
[Partition 0][Node 0] End of program
# Block size 512 x 512 x 512
# Iteration 1
$ jsrun -n192 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 4096 -Z 2048 -x 512 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 192 processes, 1 worker threads (PEs) + 0 comm threads per process, 192 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 32 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 4096 x 2048, Block: 512 x 512 x 512, Chares: 24 x 8 x 4, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.790 s
Total time: 2.561 s
Average iteration time: 25611.415 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n192 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 4096 -Z 2048 -x 512 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 192 processes, 1 worker threads (PEs) + 0 comm threads per process, 192 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 32 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 4096 x 2048, Block: 512 x 512 x 512, Chares: 24 x 8 x 4, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.722 s
Total time: 2.550 s
Average iteration time: 25497.352 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n192 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 4096 -Z 2048 -x 512 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 192 processes, 1 worker threads (PEs) + 0 comm threads per process, 192 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 32 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 4096 x 2048, Block: 512 x 512 x 512, Chares: 24 x 8 x 4, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.777 s
Total time: 2.550 s
Average iteration time: 25496.447 us
[Partition 0][Node 0] End of program
# Block size 512 x 512 x 256
# Iteration 1
$ jsrun -n192 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 4096 -Z 2048 -x 512 -y 512 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 192 processes, 1 worker threads (PEs) + 0 comm threads per process, 192 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 32 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 4096 x 2048, Block: 512 x 512 x 256, Chares: 24 x 8 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.800 s
Total time: 3.192 s
Average iteration time: 31918.970 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n192 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 4096 -Z 2048 -x 512 -y 512 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 192 processes, 1 worker threads (PEs) + 0 comm threads per process, 192 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 32 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 4096 x 2048, Block: 512 x 512 x 256, Chares: 24 x 8 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.739 s
Total time: 3.186 s
Average iteration time: 31862.411 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n192 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 4096 -Z 2048 -x 512 -y 512 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 192 processes, 1 worker threads (PEs) + 0 comm threads per process, 192 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 32 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 4096 x 2048, Block: 512 x 512 x 256, Chares: 24 x 8 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 3.021 s
Total time: 3.242 s
Average iteration time: 32419.291 us
[Partition 0][Node 0] End of program
# Block size 512 x 256 x 256
# Iteration 1
$ jsrun -n192 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 4096 -Z 2048 -x 512 -y 256 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 192 processes, 1 worker threads (PEs) + 0 comm threads per process, 192 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 32 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 4096 x 2048, Block: 512 x 256 x 256, Chares: 24 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.846 s
Total time: 3.835 s
Average iteration time: 38352.324 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n192 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 4096 -Z 2048 -x 512 -y 256 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 192 processes, 1 worker threads (PEs) + 0 comm threads per process, 192 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 32 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 4096 x 2048, Block: 512 x 256 x 256, Chares: 24 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.895 s
Total time: 3.841 s
Average iteration time: 38405.481 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n192 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 4096 -Z 2048 -x 512 -y 256 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 192 processes, 1 worker threads (PEs) + 0 comm threads per process, 192 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 32 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 4096 x 2048, Block: 512 x 256 x 256, Chares: 24 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.893 s
Total time: 3.826 s
Average iteration time: 38262.131 us
[Partition 0][Node 0] End of program

------------------------------------------------------------
Sender: LSF System <lsfadmin@batch4>
Subject: Job 277398: <jacobi3d-e-n32> in cluster <summit> Done

Job <jacobi3d-e-n32> was submitted from host <login4> by user <jchoi> in cluster <summit> at Wed Aug 12 23:39:30 2020
Job was executed on host(s) <1*batch4>, in queue <batch>, as user <jchoi> in cluster <summit> at Thu Aug 13 00:15:10 2020
                            <42*c13n05>
                            <42*c13n06>
                            <42*c13n07>
                            <42*c13n08>
                            <42*c13n09>
                            <42*c13n10>
                            <42*c13n11>
                            <42*c13n12>
                            <42*c13n13>
                            <42*c13n14>
                            <42*c13n15>
                            <42*c13n16>
                            <42*c13n17>
                            <42*c13n18>
                            <42*c25n01>
                            <42*c25n02>
                            <42*c25n03>
                            <42*c25n04>
                            <42*c25n05>
                            <42*c25n06>
                            <42*c25n07>
                            <42*c25n08>
                            <42*c25n09>
                            <42*c25n10>
                            <42*c25n11>
                            <42*c25n12>
                            <42*c25n13>
                            <42*c25n14>
                            <42*c25n15>
                            <42*c25n16>
                            <42*c25n17>
                            <42*c25n18>
</ccs/home/jchoi> was used as the home directory.
</ccs/home/jchoi/work/charm/examples/charm++/cuda/gpudirect/jacobi3d/scripts/summit> was used as the working directory.
Started at Thu Aug 13 00:15:10 2020
Terminated at Thu Aug 13 00:17:57 2020
Results reported at Thu Aug 13 00:17:57 2020

The output (if any) is above this job summary.

