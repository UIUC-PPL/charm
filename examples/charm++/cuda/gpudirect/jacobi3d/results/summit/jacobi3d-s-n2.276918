# Jacobi3D Performance Benchmarking
# Block size 1024 x 1024 x 512
# Iteration 1
$ jsrun -n12 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 3072 -Y 2048 -Z 1024 -x 1024 -y 1024 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:HybridBinomial:SHMEM:P2P
Charm++> Running in SMP mode: 12 processes, 1 worker threads (PEs) + 0 comm threads per process, 12 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 2 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.003 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 3072 x 2048 x 1024, Block: 1024 x 1024 x 512, Chares: 3 x 2 x 2, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.747 s
Total time: 4.514 s
Average iteration time: 45136.427 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n12 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 3072 -Y 2048 -Z 1024 -x 1024 -y 1024 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:HybridBinomial:SHMEM:P2P
Charm++> Running in SMP mode: 12 processes, 1 worker threads (PEs) + 0 comm threads per process, 12 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 2 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.003 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 3072 x 2048 x 1024, Block: 1024 x 1024 x 512, Chares: 3 x 2 x 2, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.706 s
Total time: 3.034 s
Average iteration time: 30337.873 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n12 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 3072 -Y 2048 -Z 1024 -x 1024 -y 1024 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:HybridBinomial:SHMEM:P2P
Charm++> Running in SMP mode: 12 processes, 1 worker threads (PEs) + 0 comm threads per process, 12 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 2 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.003 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 3072 x 2048 x 1024, Block: 1024 x 1024 x 512, Chares: 3 x 2 x 2, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.607 s
Total time: 4.516 s
Average iteration time: 45158.255 us
[Partition 0][Node 0] End of program
# Block size 1024 x 512 x 512
# Iteration 1
$ jsrun -n12 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 3072 -Y 2048 -Z 1024 -x 1024 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:HybridBinomial:SHMEM:P2P
Charm++> Running in SMP mode: 12 processes, 1 worker threads (PEs) + 0 comm threads per process, 12 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 2 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.003 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 3072 x 2048 x 1024, Block: 1024 x 512 x 512, Chares: 3 x 4 x 2, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.582 s
Total time: 3.662 s
Average iteration time: 36617.039 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n12 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 3072 -Y 2048 -Z 1024 -x 1024 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:HybridBinomial:SHMEM:P2P
Charm++> Running in SMP mode: 12 processes, 1 worker threads (PEs) + 0 comm threads per process, 12 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 2 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.003 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 3072 x 2048 x 1024, Block: 1024 x 512 x 512, Chares: 3 x 4 x 2, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.618 s
Total time: 3.546 s
Average iteration time: 35463.994 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n12 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 3072 -Y 2048 -Z 1024 -x 1024 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:HybridBinomial:SHMEM:P2P
Charm++> Running in SMP mode: 12 processes, 1 worker threads (PEs) + 0 comm threads per process, 12 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 2 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.003 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 3072 x 2048 x 1024, Block: 1024 x 512 x 512, Chares: 3 x 4 x 2, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.697 s
Total time: 3.632 s
Average iteration time: 36319.839 us
[Partition 0][Node 0] End of program
# Block size 512 x 512 x 512
# Iteration 1
$ jsrun -n12 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 3072 -Y 2048 -Z 1024 -x 512 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:HybridBinomial:SHMEM:P2P
Charm++> Running in SMP mode: 12 processes, 1 worker threads (PEs) + 0 comm threads per process, 12 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 2 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.003 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 3072 x 2048 x 1024, Block: 512 x 512 x 512, Chares: 6 x 4 x 2, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.770 s
Total time: 3.257 s
Average iteration time: 32567.789 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n12 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 3072 -Y 2048 -Z 1024 -x 512 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:HybridBinomial:SHMEM:P2P
Charm++> Running in SMP mode: 12 processes, 1 worker threads (PEs) + 0 comm threads per process, 12 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 2 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.003 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 3072 x 2048 x 1024, Block: 512 x 512 x 512, Chares: 6 x 4 x 2, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.734 s
Total time: 3.188 s
Average iteration time: 31878.868 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n12 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 3072 -Y 2048 -Z 1024 -x 512 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:HybridBinomial:SHMEM:P2P
Charm++> Running in SMP mode: 12 processes, 1 worker threads (PEs) + 0 comm threads per process, 12 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 2 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.003 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 3072 x 2048 x 1024, Block: 512 x 512 x 512, Chares: 6 x 4 x 2, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.673 s
Total time: 3.266 s
Average iteration time: 32661.959 us
[Partition 0][Node 0] End of program
# Block size 512 x 512 x 256
# Iteration 1
$ jsrun -n12 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 3072 -Y 2048 -Z 1024 -x 512 -y 512 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:HybridBinomial:SHMEM:P2P
Charm++> Running in SMP mode: 12 processes, 1 worker threads (PEs) + 0 comm threads per process, 12 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 2 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 3072 x 2048 x 1024, Block: 512 x 512 x 256, Chares: 6 x 4 x 4, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.704 s
Total time: 3.885 s
Average iteration time: 38848.820 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n12 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 3072 -Y 2048 -Z 1024 -x 512 -y 512 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:HybridBinomial:SHMEM:P2P
Charm++> Running in SMP mode: 12 processes, 1 worker threads (PEs) + 0 comm threads per process, 12 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 2 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.003 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 3072 x 2048 x 1024, Block: 512 x 512 x 256, Chares: 6 x 4 x 4, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.694 s
Total time: 3.913 s
Average iteration time: 39126.216 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n12 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 3072 -Y 2048 -Z 1024 -x 512 -y 512 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:HybridBinomial:SHMEM:P2P
Charm++> Running in SMP mode: 12 processes, 1 worker threads (PEs) + 0 comm threads per process, 12 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 2 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.003 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 3072 x 2048 x 1024, Block: 512 x 512 x 256, Chares: 6 x 4 x 4, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.853 s
Total time: 3.824 s
Average iteration time: 38236.589 us
[Partition 0][Node 0] End of program
# Block size 512 x 256 x 256
# Iteration 1
$ jsrun -n12 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 3072 -Y 2048 -Z 1024 -x 512 -y 256 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:HybridBinomial:SHMEM:P2P
Charm++> Running in SMP mode: 12 processes, 1 worker threads (PEs) + 0 comm threads per process, 12 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 2 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.003 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 3072 x 2048 x 1024, Block: 512 x 256 x 256, Chares: 6 x 8 x 4, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.785 s
Total time: 4.427 s
Average iteration time: 44269.917 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n12 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 3072 -Y 2048 -Z 1024 -x 512 -y 256 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:HybridBinomial:SHMEM:P2P
Charm++> Running in SMP mode: 12 processes, 1 worker threads (PEs) + 0 comm threads per process, 12 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 2 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.003 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 3072 x 2048 x 1024, Block: 512 x 256 x 256, Chares: 6 x 8 x 4, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.877 s
Total time: 4.447 s
Average iteration time: 44466.456 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n12 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-s -X 3072 -Y 2048 -Z 1024 -x 512 -y 256 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:HybridBinomial:SHMEM:P2P
Charm++> Running in SMP mode: 12 processes, 1 worker threads (PEs) + 0 comm threads per process, 12 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 2 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.003 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 3072 x 2048 x 1024, Block: 512 x 256 x 256, Chares: 6 x 8 x 4, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.855 s
Total time: 4.391 s
Average iteration time: 43911.492 us
[Partition 0][Node 0] End of program

------------------------------------------------------------
Sender: LSF System <lsfadmin@batch1>
Subject: Job 276918: <jacobi3d-s-n2> in cluster <summit> Done

Job <jacobi3d-s-n2> was submitted from host <login4> by user <jchoi> in cluster <summit> at Wed Aug 12 18:40:40 2020
Job was executed on host(s) <1*batch1>, in queue <batch>, as user <jchoi> in cluster <summit> at Wed Aug 12 18:40:53 2020
                            <42*g02n15>
                            <42*g05n02>
</ccs/home/jchoi> was used as the home directory.
</ccs/home/jchoi/work/charm/examples/charm++/cuda/gpudirect/jacobi3d/scripts/summit> was used as the working directory.
Started at Wed Aug 12 18:40:53 2020
Terminated at Wed Aug 12 18:43:41 2020
Results reported at Wed Aug 12 18:43:41 2020

The output (if any) is above this job summary.

