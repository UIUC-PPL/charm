# Jacobi3D Performance Benchmarking
# Block size 1024 x 1024 x 512
# Iteration 1
$ jsrun -n768 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 8192 -Z 4096 -x 1024 -y 1024 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 768 processes, 1 worker threads (PEs) + 0 comm threads per process, 768 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 128 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 8192 x 4096, Block: 1024 x 1024 x 512, Chares: 12 x 8 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.856 s
Total time: 3.725 s
Average iteration time: 37253.757 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n768 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 8192 -Z 4096 -x 1024 -y 1024 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 768 processes, 1 worker threads (PEs) + 0 comm threads per process, 768 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 128 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 8192 x 4096, Block: 1024 x 1024 x 512, Chares: 12 x 8 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.848 s
Total time: 3.723 s
Average iteration time: 37234.953 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n768 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 8192 -Z 4096 -x 1024 -y 1024 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 768 processes, 1 worker threads (PEs) + 0 comm threads per process, 768 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 128 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 8192 x 4096, Block: 1024 x 1024 x 512, Chares: 12 x 8 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.685 s
Total time: 3.694 s
Average iteration time: 36943.530 us
[Partition 0][Node 0] End of program
# Block size 1024 x 512 x 512
# Iteration 1
$ jsrun -n768 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 8192 -Z 4096 -x 1024 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 768 processes, 1 worker threads (PEs) + 0 comm threads per process, 768 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 128 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 8192 x 4096, Block: 1024 x 512 x 512, Chares: 12 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.650 s
Total time: 2.629 s
Average iteration time: 26287.846 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n768 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 8192 -Z 4096 -x 1024 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 768 processes, 1 worker threads (PEs) + 0 comm threads per process, 768 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 128 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 8192 x 4096, Block: 1024 x 512 x 512, Chares: 12 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.699 s
Total time: 2.634 s
Average iteration time: 26335.463 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n768 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 8192 -Z 4096 -x 1024 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 768 processes, 1 worker threads (PEs) + 0 comm threads per process, 768 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 128 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 8192 x 4096, Block: 1024 x 512 x 512, Chares: 12 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.657 s
Total time: 2.646 s
Average iteration time: 26457.045 us
[Partition 0][Node 0] End of program
# Block size 512 x 512 x 512
# Iteration 1
$ jsrun -n768 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 8192 -Z 4096 -x 512 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 768 processes, 1 worker threads (PEs) + 0 comm threads per process, 768 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 128 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 8192 x 4096, Block: 512 x 512 x 512, Chares: 24 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.855 s
Total time: 2.912 s
Average iteration time: 29119.885 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n768 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 8192 -Z 4096 -x 512 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 768 processes, 1 worker threads (PEs) + 0 comm threads per process, 768 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 128 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 8192 x 4096, Block: 512 x 512 x 512, Chares: 24 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.765 s
Total time: 2.920 s
Average iteration time: 29203.574 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n768 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 8192 -Z 4096 -x 512 -y 512 -z 512 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 768 processes, 1 worker threads (PEs) + 0 comm threads per process, 768 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 128 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 8192 x 4096, Block: 512 x 512 x 512, Chares: 24 x 16 x 8, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.746 s
Total time: 2.885 s
Average iteration time: 28849.493 us
[Partition 0][Node 0] End of program
# Block size 512 x 512 x 256
# Iteration 1
$ jsrun -n768 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 8192 -Z 4096 -x 512 -y 512 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 768 processes, 1 worker threads (PEs) + 0 comm threads per process, 768 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 128 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 8192 x 4096, Block: 512 x 512 x 256, Chares: 24 x 16 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.783 s
Total time: 3.521 s
Average iteration time: 35214.372 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n768 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 8192 -Z 4096 -x 512 -y 512 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 768 processes, 1 worker threads (PEs) + 0 comm threads per process, 768 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 128 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 8192 x 4096, Block: 512 x 512 x 256, Chares: 24 x 16 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.839 s
Total time: 3.493 s
Average iteration time: 34926.215 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n768 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 8192 -Z 4096 -x 512 -y 512 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 768 processes, 1 worker threads (PEs) + 0 comm threads per process, 768 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 128 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 8192 x 4096, Block: 512 x 512 x 256, Chares: 24 x 16 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.953 s
Total time: 3.513 s
Average iteration time: 35131.653 us
[Partition 0][Node 0] End of program
# Block size 512 x 256 x 256
# Iteration 1
$ jsrun -n768 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 8192 -Z 4096 -x 512 -y 256 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 768 processes, 1 worker threads (PEs) + 0 comm threads per process, 768 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 128 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 8192 x 4096, Block: 512 x 256 x 256, Chares: 24 x 32 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.988 s
Total time: 4.669 s
Average iteration time: 46689.552 us
[Partition 0][Node 0] End of program
# Iteration 2
$ jsrun -n768 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 8192 -Z 4096 -x 512 -y 256 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 768 processes, 1 worker threads (PEs) + 0 comm threads per process, 768 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 128 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 8192 x 4096, Block: 512 x 256 x 256, Chares: 24 x 32 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.894 s
Total time: 4.653 s
Average iteration time: 46528.249 us
[Partition 0][Node 0] End of program
# Iteration 3
$ jsrun -n768 -a1 -c1 -g1 -K3 -r6 ./jacobi3d-e -X 12288 -Y 8192 -Z 4096 -x 512 -y 256 -z 256 -w 10 -i 100 +ppn 1 +pemap L0,4,8,84,88,92
Choosing optimized barrier algorithm name I0:MultiLeaderBarrier:SHMEM:P2P
Charm++> Running in SMP mode: 768 processes, 1 worker threads (PEs) + 0 comm threads per process, 768 PEs total
Charm++> There's no comm. thread. Work threads both send and receive messages
Converse/Charm++ Commit ID: v6.11.0-devel-397-g5f3ed94
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> cpu affinity enabled. 
Charm++> cpuaffinity PE-core map (logical indices): 0,4,8,84,88,92
Charm++> Running on 128 hosts (2 sockets x 0 cores x 4 PUs = 4-way SMP)
Charm++> cpu topology info is gathered in 0.004 seconds.
HAPI> Config: 1 device(s) per process, 1 PE(s) per device, 6 device(s) per host
HAPI> Enabling P2P access between devices

[CUDA 2D Jacobi example]
Grid: 12288 x 8192 x 4096, Block: 512 x 256 x 256, Chares: 24 x 32 x 16, Iterations: 100, Warm-up: 10, Bulk-synchronous: 0, Zerocopy: 0, Print: 0

Init time: 2.931 s
Total time: 4.695 s
Average iteration time: 46950.530 us
[Partition 0][Node 0] End of program

------------------------------------------------------------
Sender: LSF System <lsfadmin@batch5>
Subject: Job 277404: <jacobi3d-e-n128> in cluster <summit> Done

Job <jacobi3d-e-n128> was submitted from host <login4> by user <jchoi> in cluster <summit> at Wed Aug 12 23:40:50 2020
Job was executed on host(s) <1*batch5>, in queue <batch>, as user <jchoi> in cluster <summit> at Thu Aug 13 00:21:02 2020
                            <42*b25n04>
                            <42*b25n05>
                            <42*b25n06>
                            <42*b25n07>
                            <42*b25n08>
                            <42*b25n09>
                            <42*b25n10>
                            <42*b25n11>
                            <42*b25n12>
                            <42*b25n13>
                            <42*b25n14>
                            <42*b25n15>
                            <42*b25n16>
                            <42*b25n17>
                            <42*b25n18>
                            <42*b26n01>
                            <42*b26n02>
                            <42*b26n03>
                            <42*b26n04>
                            <42*b26n05>
                            <42*b26n06>
                            <42*b26n07>
                            <42*b26n08>
                            <42*b26n09>
                            <42*b26n10>
                            <42*b26n11>
                            <42*b26n12>
                            <42*b26n13>
                            <42*b26n14>
                            <42*b26n15>
                            <42*b26n16>
                            <42*b26n17>
                            <42*b26n18>
                            <42*b27n01>
                            <42*b27n02>
                            <42*b27n03>
                            <42*b27n04>
                            <42*b27n05>
                            <42*b27n06>
                            <42*b27n07>
                            <42*b27n08>
                            <42*b27n09>
                            <42*b27n10>
                            <42*b27n11>
                            <42*b27n12>
                            <42*b27n13>
                            <42*b27n14>
                            <42*b27n15>
                            <42*b27n16>
                            <42*b27n17>
                            <42*b27n18>
                            <42*b28n01>
                            <42*b28n02>
                            <42*b28n03>
                            <42*b28n04>
                            <42*b28n05>
                            <42*b28n06>
                            <42*b28n07>
                            <42*b28n08>
                            <42*b28n09>
                            <42*b28n10>
                            <42*b28n11>
                            <42*b28n12>
                            <42*b28n13>
                            <42*b28n14>
                            <42*b28n15>
                            <42*b28n16>
                            <42*b28n17>
                            <42*b28n18>
                            <42*b29n01>
                            <42*b29n02>
                            <42*b29n03>
                            <42*b29n04>
                            <42*b29n05>
                            <42*b29n06>
                            <42*b29n07>
                            <42*b29n08>
                            <42*b29n09>
                            <42*b29n10>
                            <42*b29n11>
                            <42*b29n12>
                            <42*b29n13>
                            <42*b29n14>
                            <42*b29n15>
                            <42*b29n16>
                            <42*b29n17>
                            <42*b29n18>
                            <42*b30n01>
                            <42*b30n02>
                            <42*b30n03>
                            <42*b30n04>
                            <42*b30n05>
                            <42*b30n06>
                            <42*b30n07>
                            <42*b30n08>
                            <42*b30n09>
                            <42*b30n10>
                            <42*b30n11>
                            <42*b30n12>
                            <42*b30n13>
                            <42*b30n14>
                            <42*b30n15>
                            <42*b30n16>
                            <42*b30n17>
                            <42*b30n18>
                            <42*b31n01>
                            <42*b31n02>
                            <42*b31n03>
                            <42*b31n04>
                            <42*b31n05>
                            <42*b31n06>
                            <42*b31n07>
                            <42*b31n08>
                            <42*b31n09>
                            <42*b31n10>
                            <42*b31n11>
                            <42*b31n12>
                            <42*b31n13>
                            <42*b31n14>
                            <42*b31n15>
                            <42*b31n16>
                            <42*b31n17>
                            <42*b31n18>
                            <42*b32n01>
                            <42*b32n02>
                            <42*b32n03>
                            <42*b32n04>
                            <42*b32n05>
</ccs/home/jchoi> was used as the home directory.
</ccs/home/jchoi/work/charm/examples/charm++/cuda/gpudirect/jacobi3d/scripts/summit> was used as the working directory.
Started at Thu Aug 13 00:21:02 2020
Terminated at Thu Aug 13 00:24:02 2020
Results reported at Thu Aug 13 00:24:02 2020

The output (if any) is above this job summary.

